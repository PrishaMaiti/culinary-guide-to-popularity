# The Culinary Guide to Popularity

## Introduction
This project aims to answer what types of recipes tend to have higher ratings on average than others. For avid chefs at home looking to share their recipes online for the world to follow, these results will come in handy to understand what recipe followers want most in their food! Using a subset of data from food.com, I utilized hundreds of thousands of recipes and ratings to find the factors of recipes that most heavily influence what give the recipe a high rating. The dataset I used was a merge of two datasets, one on recipes, and another one on ratings for those recipes, from food.com, as well as some new columns I added for analysis. The dataset has a total of 83782 rows and 20 columns. The columns I used are: 'id', which is the id of each recipe, 'n_steps', which is the number of steps to follow for the recipe, 'n_ingredients', which is the number of ingredients required by the recipe, 'average_rating', which is the average of all ratings corresponding to the same recipe ID, 'tag_count', which is the number of tags a recipe has on food.com such as "60-minutes-or-less" or "for-large-groups", and 'calories', which is the recipe's amount of calories and is a result of extracting the first item from the lists under the 'nutrition' column, which gives information about the recipe's nutritional value.

## Data Cleaning and Exploratory Data Analysis

In order to preprocess the data so that it was ready for analysis, I first had to combine the two original datasets, recipes and ratings, into one. Once I merged the two by matching with recipe_id's, I noticed many ratings were 0, which didn't make sense practically. Instead, I replaced those 0's with NaN's to indicate that no one has rated those particular recipes yet. The last thing I did was add a new column called `average_rating`, which was the target variable I intended to explore by seeing how other variables in other columns affected it. I did it by grouping the data by recipe_id, and taking the average rating of all ratings corresponding to the same recipe_id, so that each unique recipe would get an average rating. Additionally, I cleaned the 'nutrition' column. The original values in that column were strings that looked like lists, so I had to convert them into lists to be able to use them for future analysis. Additionally, since each value in the lists represented a different piece of information about the recipe's nutrition (first value was number of calories, second value was total fat (PDV), and so on for a total of 7 values), I expanded the dataset so that it included separate columns for each type of information, while still keeping the original nutrition list. The picture below shows what the cleaned data looks like:
<img width="847" height="164" alt="Screenshot 2025-12-09 at 12 01 23 PM" src="https://github.com/user-attachments/assets/fe6755da-4d11-4353-9daa-baa20d480926" />

I conducted three univariate analyses to see the distribution of three variables: average rating, number of steps, and number of ingredients. The latter two were variables I believed were useful to accurately predict a recipe's average rating, so I wanted to include them in my initial analysis. The plot below shows the distribution of the number of steps, which appear to have a moderately strong skew to the right, meaning that most recipes don't require as many steps as the average number of steps required to follow in these recipes.
<iframe
  src="assets/n_steps_dist.html"
  width="800"
  height="600"
  frameborder="0"
></iframe>


The plot below displays the relationship between average rating and number of steps. It looks like the bulk of average ratings fall between the range of 4-5, and also typically require no more than 40 steps.
<iframe
  src="assets/avg_rating_vs_n_steps.html"
  width="800"
  height="600"
  frameborder="0"
></iframe>


I created a pivot table to explore the average number of tags given the number of ingredients for each unique number of steps. This is a useful way to quickly explore how recipes with different numbers of ingredients and different numbers of steps may fall into more or less categories for what the food can be used for, whether it's a quick source of protein, or a large family size meal to take to potlucks!
<img width="523" height="236" alt="Screenshot 2025-12-08 at 9 29 02 PM" src="https://github.com/user-attachments/assets/8da1c141-f4b9-43e7-87d4-b2168492e315" />

## Assessment of Missingness

I found out which columns in my dataset contained any missing values. There were three: name, description, and my aggregated column average_rating. `name` had just 1 missing value and `description` had 70 missing values. `average_rating` however, had 2609 missing values, which couldn't have been just a coincidence. So I decided to test whether the missingness of the `average_rating` column was not missing at random (NMAR), that is, whether the values in the column were missing because of the variable `average_rating` itself, or if the missingness depended on another variable. I decided to conduct a permutation tes to determine whether the missingness of `average_rating` depended on `n_steps`, the number of steps in a recipe. After doing the test, I got a p-value of close to 0. A **p-value** tells us the probability that we'd get results as extreme as the ones we observed due to random chance. Typically, a common threshold is 0.05, so when our p-value is < 0.05, we reject the null hypothesis, which is that the missing `average_rating` values are NMAR, or don't depend on any other variable. That means we have sufficient evidence in favor of our alternative hypothesis, which is that the data is missing at random (MAR), or that it depends on `n_steps`.
TODO: Plotly plot coming here soon!

## Hypothesis Testing

To continue my exploration of how average recipe rating worked in different situations, I decided to test whether the average ratings for **simple recipes** (requiring <= 5 steps) and **complex recipes** (requiring > 5 steps) were the same. That was my null hypothesis, and my alternative hypothesis was that the mean average rating of simple recipes is higher than the mean average rating of complex recipes, because it is possible that people, due to laziness or having a tight schedule, might rate a recipe higher than another because it requires less tasks to follow. So the test statistic I used was the difference in means between simple and complex recipes, and I compared the p-value to the standard 0.05 significance level. The p-value I got was 0.008 < 0.05, which tells us that the observed difference in means is statistically significant. We reject the null hypothesis, as there is sufficient evidence that simple recipes have a higher mean rating than complex recipes.

## Framing a Prediction Problem

Continuing with the work I've done on exploring the relationship between average rating and other variables in the dataset, I decided to build a **linear regression** machine learning model that predicts the average rating of a recipe. Therefore, it made sense that average rating was my response variables while other columns in the dataset whose data is already recorded were used as explanatory variables or **features**, such as the number of steps or number of ingredients used in a recipe. I used **root mean squared error** (RMSE) as my metric to evaluate the model's performance, because I figured a good way to know if a model is performing optimally is if the error can be reduced as much as possible.

## Baseline Model

As a start to predicting the average rating of recipes, I created a linear regression model using two main features, number of steps and number of ingredients. Both features are numerical by nature, so I did not have to encode any variable to make it useful toward the predictions, with the exception of standard scaling of units through scikit-learn's StandardScaler() transformer. Since I split my data into 80% for training and 20% for testing, I got two RMSE's, one for training and the other for testing. I got a training RMSE of 0.6419296 and a testing RMSE of 0.6359896. Since a good model should have a balanced training and testing RMSE, I'd say this was a good start, but I hoped to reduce them even further by including more features in my model.

## Final Model

To improve the model's performance, i.e. reduce the RMSE, I added several new features: `tag_count`, which represents the number of items in each list under the `tags` column, and `calories`, which was an extraction of the first item of every list under the `nutrition` column, since that was designed to contain the total number of calories in the recipe. All these new features are numerical, so once again, I didn't have to do any sort of categorical encoding. The reason I chose to add these two features in particular was that, practically, a recipe might not just be determined by how many steps it takes or how many ingredients it requires. Flexibility of a recipe for people with different dietary restrictions is also crucial (more tags means more categories the recipe falls under for different situations), as well as number of calories for health-conscious people who may be calorie counting on a daily basis. Additionally, I changed the model from linear regression to **DecisionTreeRegressor** so that I could tune some hyperparameters of the model before feeding data. The hyperparameters I decided to tune were: `max_depth`, `min_samples_leaf`, and `min_samples_split`. I chose `max_depth` because it controls how deep the decision tree can grow, making sure it's not too deep to the point where we're overfitting, or too shallow to the point where we're underfitting. I chose `min_samples_leaf` because we need a large enough number of samples in a leaf node for the tree to be able to generalize better. If we have a smaller count, we would tend to get an entire dedicated leaf for just one sample even if it is a little bit different from the other samples, which often leads to overfitting. Similarly, `min_samples_split` allows us to control the minimum number of samples needed to split a node. Larger values mean fewer splits which means less complexity, smaller values mean more splits which means more complexity, so the key is to strike a good balance to avoid under or overfitting. Using sklearn's `GridSearchCV`, I programatically found the optimal combination of hyperparameter values, which was 3 for `max_depth`, `5` for min_samples_leaf, and 2 for `min_samples_split`. Like in the baseline model, I similarly constructed a pipeline to make predictions and compute a training and testing RMSE, which were 0.6415974 and 0.6360676, respectively. This is an improvement from the baseline model in the sense that we slightly reduced the taining RMSE but slightly increased the testing RMSE. There is still a balance between both RMSE's, which is a sign that this is also a good model. A lower training RMSE would typically reduce overfitting while a lower testing RMSE typically would allow the model to generalize better, or make better predictions. So although we might have reduced the model's accuracy on the specific testing data, we did reduce the chance of the model overfitting on the training data by striking a balance between the two RMSE's, which can still be interpreted as this new model does a better job at predicting the average rating for recipes!

## Fairness Analysis

While the second model does have a lower RMSE than the first, I had to consider whether my model performed worse for simple recipes than complex recipes as I was initially exploring in my permutation test.
